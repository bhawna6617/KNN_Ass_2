{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e221a053",
   "metadata": {},
   "source": [
    "# quest 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ca5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main difference between the Euclidean distance metric and the Manhattan distance metric lies in how they calculate the distance between data points.\n",
    "\n",
    "# Euclidean distance measures the straight-line distance between two points in a Euclidean space (i.e., the shortest path between two points). It is calculated using the square root of the sum of the squared differences between corresponding coordinates.\n",
    "\n",
    "# Manhattan distance measures the distance between two points by summing the absolute differences of their coordinates. It is named so because it measures the distance a pedestrian would walk along a city block.\n",
    "\n",
    "# The difference in calculation methods can affect the performance of a KNN classifier or regressor in several ways:\n",
    "\n",
    "# Sensitivity to Feature Scaling: Euclidean distance is sensitive to differences in scale between features, as it takes into account the squared differences. In contrast, Manhattan distance is less sensitive to feature scaling because it considers the absolute differences. Therefore, if features have significantly different scales, Euclidean distance may lead to biased results, while Manhattan distance may be more robust.\n",
    "\n",
    "# Impact of Outliers: Manhattan distance is more robust to outliers compared to Euclidean distance. Outliers have a greater impact on Euclidean distance because of the squared differences, while Manhattan distance only considers absolute differences. Therefore, in the presence of outliers, Manhattan distance may lead to more stable results.\n",
    "\n",
    "# Feature Interpretability: Manhattan distance can be more interpretable in some cases because it measures distances along coordinate axes, making it easier to understand the contribution of each feature to the overall distance between points. Euclidean distance, on the other hand, considers distances in all dimensions equally.\n",
    "\n",
    "# Computational Efficiency: In high-dimensional spaces, computing Euclidean distance involves calculating square roots and summing squared differences, which can be computationally expensive compared to Manhattan distance, which involves summing absolute differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac79ece",
   "metadata": {},
   "source": [
    "# quest 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4756420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the optimal value ofk for a KNN classifier or regressor is crucial for achieving good performance and generalization on unseen data. Several techniques can be used to determine the optimal \n",
    "\n",
    "# k value:\n",
    "\n",
    "# Cross-Validation: One of the most common techniques is k-fold cross-validation. In this approach, the dataset is divided into \n",
    "# k subsets (folds), and the model is trained and evaluated \n",
    "# k times, each time using a different fold as the validation set and the remaining folds as the training set. The average performance across all folds is then calculated for each \n",
    "# k value, and the one with the highest performance is selected as the optimal \n",
    "# k value.\n",
    "\n",
    "# Grid Search: Grid search involves evaluating the model's performance for different combinations of hyperparameters, including different values of \n",
    "# k. The model is trained and evaluated for each combination of hyperparameters, and the combination that results in the best performance, as measured by a chosen evaluation metric (e.g., accuracy, F1 score, mean squared error), is selected.\n",
    "\n",
    "# Elbow Method: For regression tasks, the elbow method can be used to visually identify the optimal \n",
    "# k value. In this approach, the mean squared error (or another appropriate error metric) is calculated for different \n",
    "# ùëò values, and a plot of the error metric against \n",
    "# k is generated. The point where the error starts to decrease at a slower rate (resembling an \"elbow\" shape) is often chosen as the optimal \n",
    "# k value.\n",
    "\n",
    "# Leave-One-Out Cross-Validation (LOOCV): LOOCV is a special case of cross-validation where \n",
    "# k is set to the number of samples in the dataset. The model is trained on \n",
    "# n‚àí1 samples and tested on the remaining sample, repeating this process \n",
    "# n times (once for each sample). The average performance across all iterations is then used to select the optimal \n",
    "# k value.\n",
    "\n",
    "# Domain Knowledge and Experimentation: Sometimes, domain knowledge about the problem can provide insights into the appropriate range of \n",
    "# k values. Experimenting with different values of \n",
    "# k and observing the model's performance on validation data can also help in selecting the optimal \n",
    "# k value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce3daf",
   "metadata": {},
   "source": [
    "#  quest 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c8313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The choice of distance metric significantly affects the performance of a KNN classifier or regressor because it determines how similarity between data points is calculated. Different distance metrics may lead to different results and can impact the model's ability to generalize to unseen data. Here's how the choice of distance metric can influence performance and situations where one metric might be preferred over the other:\n",
    "\n",
    "# Euclidean Distance:\n",
    "\n",
    "# Advantages:\n",
    "# Works well when features are continuous and have a Gaussian distribution.\n",
    "# Suitable for tasks where the spatial relationship between data points is important.\n",
    "# Disadvantages:\n",
    "# Sensitive to differences in feature scales. Features with larger scales can dominate the distance calculation.\n",
    "# May not perform well with high-dimensional data due to the curse of dimensionality.\n",
    "# Situations to Choose:\n",
    "# When feature scaling is applied or when features have similar scales.\n",
    "# For tasks where the Euclidean space accurately represents the underlying data distribution.\n",
    "# Manhattan Distance (also known as Taxicab or City Block Distance):\n",
    "\n",
    "# Advantages:\n",
    "# Less sensitive to differences in feature scales compared to Euclidean distance.\n",
    "# Effective for datasets with categorical or ordinal features.\n",
    "# Suitable for high-dimensional data due to its insensitivity to the curse of dimensionality.\n",
    "# Disadvantages:\n",
    "# May not capture the underlying spatial relationships between data points as effectively as Euclidean distance.\n",
    "# Situations to Choose:\n",
    "# When dealing with categorical or ordinal features.\n",
    "# For tasks where the relative importance of features is known and the Manhattan distance aligns with that importance.\n",
    "# When working with high-dimensional data where Euclidean distance may be less effective due to the curse of dimensionality.\n",
    "# Other Distance Metrics:\n",
    "\n",
    "# Minkowski Distance: A generalization of both Euclidean and Manhattan distances, where the choice of parameter \n",
    "# p determines the behavior of the distance metric. It can interpolate between Euclidean and Manhattan distances based on the value of \n",
    "# p.\n",
    "# Cosine Similarity: Measures the cosine of the angle between two vectors, which is often used in text analysis or when dealing with high-dimensional sparse data such as word embeddings.\n",
    "# Hamming Distance: Specifically for binary data or categorical variables, measuring the number of differing bits or categories between two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e9808",
   "metadata": {},
   "source": [
    "# quest 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22bbde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In KNN classifiers and regressors, several hyperparameters can significantly impact the model's performance. Here are some common hyperparameters and their effects:\n",
    "\n",
    "# Number of Neighbors (\n",
    "# ùëò\n",
    "# k): This hyperparameter determines the number of nearest neighbors considered when making predictions.\n",
    "\n",
    "# Effect: Larger values of \n",
    "# ùëò\n",
    "# k lead to smoother decision boundaries and can reduce the effect of noise in the data but may sacrifice model sensitivity to local patterns. Smaller values of \n",
    "# ùëò\n",
    "# k can capture finer details in the data but may be sensitive to noise.\n",
    "# Tuning: Use techniques like cross-validation or grid search to find the optimal \n",
    "# ùëò\n",
    "# k value that balances bias and variance.\n",
    "# Distance Metric: The choice of distance metric (e.g., Euclidean, Manhattan, Minkowski) determines how similarity between data points is calculated.\n",
    "\n",
    "# Effect: Different distance metrics can lead to different notions of similarity between data points, impacting the model's performance.\n",
    "# Tuning: Experiment with different distance metrics and choose the one that best fits the data distribution and problem requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c81d5",
   "metadata": {},
   "source": [
    "# quest 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7110e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect on Bias and Variance:\n",
    "\n",
    "# Small Training Set: With a small training set, the model may suffer from high bias because it lacks sufficient information to capture the underlying patterns in the data. This can lead to underfitting.\n",
    "# Large Training Set: A large training set can help reduce bias by providing more representative samples of the underlying data distribution. However, if the training set is too large, the model may suffer from high variance, as it may memorize noise or irrelevant patterns in the data, leading to overfitting.\n",
    "# Computational Complexity:\n",
    "\n",
    "# Small Training Set: Training with a small dataset may be computationally efficient but may result in suboptimal performance due to limited information.\n",
    "# Large Training Set: Training with a large dataset can increase computational complexity and training time, especially in KNN, where prediction involves calculating distances to all training samples.\n",
    "# Generalization:\n",
    "\n",
    "# Small Training Set: Models trained on small datasets may not generalize well to unseen data, as they may not capture the true underlying patterns in the data.\n",
    "# Large Training Set: Models trained on large datasets are more likely to generalize well to unseen data, as they have been exposed to a wider range of examples.\n",
    "# To optimize the size of the training set and improve the performance of a KNN model:\n",
    "\n",
    "# Cross-Validation: Use techniques like k-fold cross-validation to evaluate the model's performance across different training set sizes. This can help identify the optimal size that balances bias and variance.\n",
    "\n",
    "# Learning Curves: Plot learning curves to visualize the model's performance as a function of the training set size. This can help identify whether the model would benefit from additional data or if it has already reached its performance plateau.\n",
    "\n",
    "# Data Augmentation: If obtaining more data is not feasible, consider data augmentation techniques to artificially increase the size of the training set. This could include techniques such as adding noise, rotating, flipping, or cropping images, or generating synthetic data points.\n",
    "\n",
    "# Feature Selection/Dimensionality Reduction: If the dataset is large but contains many irrelevant or redundant features, consider performing feature selection or dimensionality reduction techniques (e.g., PCA) to reduce the dimensionality of the dataset and improve the model's performance.\n",
    "\n",
    "# Sampling Techniques: If the dataset is imbalanced, use sampling techniques such as oversampling or undersampling to balance the class distribution and improve model performance.\n",
    "\n",
    "# Incremental Learning: For large datasets that cannot fit into memory, consider using incremental learning techniques that train the model on small batches of data sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d687d47",
   "metadata": {},
   "source": [
    "# quest 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bde12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While KNN is a simple and intuitive algorithm, it does have some drawbacks that can affect its performance in certain situations:\n",
    "\n",
    "# Computational Complexity: KNN's prediction time complexity is \n",
    "# O(n‚ãÖm), where \n",
    "\n",
    "# n is the number of training samples and \n",
    "\n",
    "# m is the number of features. This makes it computationally expensive, especially for large datasets and high-dimensional feature spaces.\n",
    "\n",
    "# Mitigation: To reduce computational complexity, techniques like approximate nearest neighbor search, KD-trees, or ball trees can be used to speed up the search for nearest neighbors.\n",
    "\n",
    "# Memory Consumption: KNN requires storing the entire training dataset in memory for prediction, which can be memory-intensive, particularly for large datasets.\n",
    "\n",
    "# Mitigation: Techniques like pruning or compression methods can be used to reduce memory consumption while maintaining the necessary information for prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
